{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Capstone Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "Road Accidents have always been a major and severe cause/contributor in accidental deaths. They have been increasing in severity and frequency due to the proliferation of more sophisticated and advanced vehicles that can reach upto phenomenal speeds on the highways. There are a lot of external factors that have the potential of aggravating road accidents' severity and resulting casualities. The varying road conditions, weather conditions, pecularities of particular locations and traffic signs and systems are some of them. There is a persisting opportunity gap that can be leveraged by using past data about road accidents and collisions to build a prediction system of some sorts that can point in the direction of possible accidental conditions, especially the severity or the possibility of the extent of damage, and can help save lives. I believe, such a solution would be appealing to not just the concerned people themselves, but also to the emergency services, medical facilites and institutions, highway authorities and the government. This solution will guide the aforementioned authorities to be better and more appropriately prepared(depending on the severity of the accident) for any possible calamities that might occur on the road in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the most appropriate measures that could be taken to prevent the road accidents or shorten the response time of arrival of medical help or other sorts of emergency services could be developed from observing, analyzing and processing past accidental data on the roads that have taken over a considerable period of time. The dataset is hosted by the City of Seattle. It includes all kinds of collisions as updated and reported by Seattle Department of Traffic(SDOT) for the timeframe of 2014 till present.\n",
    "The dataset contains approximately 200,000 instances of collision details from road accidents, including the details about the location, time and other relavant conditions leading upto accidents. It contains 38 features:\n",
    "- 'X',\n",
    "- 'Y',\n",
    "- 'OBJECTID',\n",
    "- 'INCKEY',\n",
    "- 'COLDETKEY',\n",
    "- 'REPORTNO',\n",
    "- 'STATUS',\n",
    "- 'ADDRTYPE',\n",
    "- 'INTKEY',\n",
    "- 'LOCATION',\n",
    "- 'EXCEPTRSNCODE',\n",
    "- 'EXCEPTRSNDESC',\n",
    "- 'SEVERITYCODE',\n",
    "- 'SEVERITYDESC',\n",
    "- 'COLLISIONTYPE',\n",
    "- 'PERSONCOUNT',\n",
    "- 'PEDCOUNT',\n",
    "- 'PEDCYLCOUNT',\n",
    "- 'VEHCOUNT',\n",
    "- 'INCDATE',\n",
    "- 'INCDTTM',\n",
    "- 'JUNCTIONTYPE',\n",
    "- 'SDOT_COLCODE',\n",
    "- 'SDOT_COLDESC',\n",
    "- 'INATTENTIONIND',\n",
    "- 'UNDERINFL',\n",
    "- 'WEATHER',\n",
    "- 'ROADCOND',\n",
    "- 'LIGHTCOND',\n",
    "- 'PEDROWNOTGRNT',\n",
    "- 'SDOTCOLNUM',\n",
    "- 'SPEEDING',\n",
    "- 'ST_COLCODE',\n",
    "- 'ST_COLDESC',\n",
    "- 'SEGLANEKEY',\n",
    "- 'CROSSWALKKEY',\n",
    "- 'HITPARKEDCAR'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After understanding the business requirements, needs and opportunity gaps, along with getting an overview of the dataset, it can be established that the problem can be mapped to a Machine Learning Classification problem. A thorough andn detailed exploration of the data set to analyze and extract the patterns and insights to reveal more information about the causes and severity of road accidents. A Classification model can be used on top of that to make necessary predictions regarding the severity of the accident.\n",
    "\n",
    "#### Data Cleaning\n",
    "I started off with cleaning and wrangling the data. There were certain columns that were redundant. After removing those columns, next in line were columns which had more than 50% missing values. Most of the other columns that had any missing values had them for less than 5% of the whole datase, so I removed the rows which had those. After cleaning the data, it was time to dive deep into it.\n",
    "\n",
    "#### Exploratory Data Analysis\n",
    "I started with exploratory data analysis by analyzing the data through Univariate, Bivariate and Multivariate Analysis to reveal initial insights and lay the groundwork of choosing the right model for Classification. This phase of data wrangling included multiple plots of features against each other and against the target variable as well.\n",
    "\n",
    "#### Feature Engineering\n",
    "I introduced a couple of new columns from the Date of Incident columns to feed the model with more relavant information in terms of day of week and the month of the collisions/accidents. This would be relevant while training the model as the model will have better, and more revealing knowledge about the incidents.\n",
    "\n",
    "#### Data Preprocessing\n",
    "After the aforementioned steps, it was time to scale the dataset, so that the model doesnt get biased towards numerical values that are significantly larger than the rest. Also, I handled and dealt with the categorical variables in this phase. There were around 7 Categorical variables in the dataset left after executing the above steps. I used One-Hot encoding to transform those into numerical, so that they can be digested by the model.\n",
    "\n",
    "#### Model Building\n",
    "The data set had around 70 features after encoding. As a mandatory primary step in this phase, I began with splitting the dataset into training and testing data i the ratio of 70/30 respectively. Then, I used feature selection technique called Recursive Feature Elimination to select 25 columns out of 70 that are most relavant and have considerable significance over the predictive power of the model.\n",
    "\n",
    "Generally, one-hot encoded data is highly sparse and is susceptible to Multicollinearity. So, I used Varince Inflation Factor to check for any. I removed 2 more columns that were highly redundant and finally build the first model with Logistic Regression. I then tried Support Vector Machines with rbf kernel and inally did Hyperparameter tuning in the Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy reported by the best Classifier was 75%.\n",
    "I tested the model on some other metrics including:\n",
    "- ROC-AUC-Score : 0.79\n",
    "- Recall: 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While doing Exploratory data analysis and further Modeling, I noted a few important things that became evident through different plots:\n",
    "\n",
    "- The majority of accidents/collisions occur on blocks rather than intersections. Although, the accidents occuring on intersections are usually more severe than others. I believe this particular insight could be leveraged in the future to allocate relevant resources around respective areas in order to be better prepared and reduce the response time.\n",
    "- The results of the analysis could also prove beneficial to the planning and implementation of parking spaces and pedestrian rules regarding crossing, etc as the data shows high collisions with parked cars.\n",
    "- Most of the severe accidents occuring on the intersections are at angles and in high speeds, especially during left turns. The trraffic authorities could use this insight to better plan and design intersections and crossings.\n",
    "- Most of the accidents and severe collisions have taken place during the day between 4 and 6 PM on Mondays and Fridays. The emergency services, medical facilities and other concerned authorities could structure their schedules around this time to be better prepared.\n",
    "- The model can predict with 75% accuracy regarding the severity of any accident that gets reported. Appropriate measures can be taken depending upon the prediction of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the methodology section delineates and proposes the solution for the earlier mentioned business problem. Leveraging the concepts of Data Science and Machnine Learning and using the power of data, we can take better precautions and be better positioned to take more suitable and appropriate measures and can help save lives. However, the solution provided is not the best solution that can be achieved. More data will lead to better predictive power of the models and can set us off in the direction of using even more sophisticated Machine Learning algorithms. The solution would be reconsidered and reiterated with suitable alternatives and new enhancements that will facilitate the better usage of data and lead to even better evaluation metrics.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
